{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza dos microdados do enem 2022\n",
    "- Neste notebook, realizarei a limpeza dos microdados do enem 2022.\n",
    "- Os microdados se constituem no menor nível de desagregação de dados recolhidos por pesquisas, avaliações e exames realizados. No caso do Enem, os dados estão por participante.\n",
    "- A limpeza de dados é necessária para performar a análise exploratória de dados. Dado o alto volume de dados e a sua origem (dados reais), algumas tarefas devem ser realizadas.\n",
    "- Estão incluídas nessas tarefas:\n",
    "    -  Identificação e tratamento de valores nulos e duplicados, de acordo com os objetivos da análise.\n",
    "    -  Remoção de variáveis irrelevantes para a análise.\n",
    "    -  Feature engineering: Criação e alteração de variáveis existentes. Aqui, irei fundir, remover e renomear categorias com base na melhor formatação para o meu objetivo. Além disso, converter colunas para o tipo de dado correto também será importante.\n",
    "    -  Otimização de memória: Conversão de variáveis a tipos de dados menores, a fim de melhorar a performance, possibilitando a leitura e manipulação dos dados em menor tempo, sem que haja a perda de informação.\n",
    "- Irei efetuar duas análises após a limpeza. O objetivo de cada uma delas guiará decisões tomadas futuramente neste notebook.\n",
    "- Na Análise de Desempenho, tenho como foco analisar o perfil de candidatos que obtêm determinadas notas, quais variáveis se relacionam com as notas e como estas poderiam ser utilizadas para predição. Portanto, é importante utilizar apenas dados de estudantes que estavam presentes em ambos os dias de prova, que de fato obtiveram um resultado. Incluir todos os alunos introduziria assimetrias e distorções na análise.\n",
    "- Na Análise de Abstenção, tenho como foco analisar quais os fatores que influenciam o comparecimento (ou não) do candidato. Portanto, é necessário incluir todos os alunos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and visualization.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Coletando os dados\n",
    "- Considerando o alto volume de dados, irei ler o dataset microdados em \"chunks\", partes menores, unindo tudo ao fim. Isso tornará a leitura mais eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading large data in separate chunks, then concatening into a single dataframe again.\n",
    "chunk_size = 50_000\n",
    "chunks = []\n",
    "microdados_path = \"D:\\MLProjects\\DadosEnem\\MICRODADOS_ENEM_2022.csv\"\n",
    "for chunk in pd.read_csv(microdados_path, sep=';', encoding='ISO-8859-1', chunksize=chunk_size):\n",
    "   chunks.append(chunk)\n",
    "\n",
    "microdados = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Entendimento inicial dos dados e limpeza dos dados\n",
    "- Nesta etapa irei observar superficialmente os dados, obtendo dimensões, tipos de dados das variáveis, valores nulos e duplicados, estatísticas descritivas, entre outros.\n",
    "- Será realizada a limpeza deles também. Irei remover colunas desnecessárias, converter variáveis para os tipos de dado corretos, reduzir o tamanho do dataset, tratar valores nulos e outliers, entre outras tarefas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Dicionário de variáveis\n",
    "- O dicionário de variáveis encontra-se em 'input/dictionary/Dicionário_Microdados_Enem_2022.xslx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Informações gerais sobre os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microdados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microdados.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'O dataset possui {microdados.shape[0]} linhas e {microdados.shape[1]} colunas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Valores nulos e duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microdados['NU_INSCRICAO'].duplicated().sum()\n",
    "microdados.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = (microdados.isna().sum() / len(microdados) * 100).to_frame().rename(columns={0: 'null_pct'})\n",
    "null_df['null_count'] = microdados.isna().sum()\n",
    "null_df.sort_values(by=['null_pct'], ascending=False).head(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Não há observações duplicadas no dataset.\n",
    "- Variáveis referentes à escola possuem um alto percentual de nulos, acima de 70%, e portanto deverão ser removidas. Da mesma forma, a variável TP_ENSINO.\n",
    "- É possível ver um padrão nas variáveis referentes à nota e código de prova, possuindo o mesmo percentual de nulos para cada dia de aplicação (32% para as provas de ciências da natureza e matemática e 28.2% para as provas de ciências humanas, linguagens e suas tecnologias e redação). Provavelmente esses valores nulos representam alunos que não compareceram ou que foram eliminados, vamos investigar adiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for null values in the days when the student was not present for the second day of the exam.\n",
    "segundo_dia = ['CO_PROVA_CN',\n",
    " 'NU_NOTA_CN',\n",
    " 'TX_RESPOSTAS_CN',\n",
    " 'TX_GABARITO_CN',\n",
    " 'CO_PROVA_MT',\n",
    " 'NU_NOTA_MT',\n",
    " 'TX_RESPOSTAS_MT',\n",
    " 'TX_GABARITO_MT']\n",
    "microdados[segundo_dia].loc[(microdados['TP_PRESENCA_MT'] == 0) & (microdados['TP_PRESENCA_CN'] == 0)].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for null values in the days when the student was eliminated for the second day of the exam.\n",
    "microdados[segundo_dia].loc[(microdados['TP_PRESENCA_MT'] == 2) & (microdados['TP_PRESENCA_CN'] == 2)].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for null values in the days when the student was not present for the first day of the exam.\n",
    "primeiro_dia = ['CO_PROVA_CH', 'CO_PROVA_LC', 'NU_NOTA_CH', 'NU_NOTA_LC',\n",
    "       'TX_RESPOSTAS_CH', 'TX_RESPOSTAS_LC', 'TX_GABARITO_CH',\n",
    "       'TX_GABARITO_LC', 'TP_STATUS_REDACAO', 'NU_NOTA_COMP1', 'NU_NOTA_COMP2',\n",
    "       'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5', 'NU_NOTA_REDACAO']\n",
    "microdados[primeiro_dia].loc[(microdados['TP_PRESENCA_CH'] == 0) & (microdados['TP_PRESENCA_LC'] == 0)].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for null values in the days when the student was eliminated for the first day of the exam.\n",
    "microdados[primeiro_dia].loc[(microdados['TP_PRESENCA_CH'] == 2) & (microdados['TP_PRESENCA_LC'] == 2)].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microdados['TP_PRESENCA_LC'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microdados['TP_PRESENCA_MT'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- É possível perceber que todas as observações que contêm valores nulos em variáveis contendo informações sobre as provas ocorrem por não comparecimento ou eliminação do estudante.\n",
    "- Considerando que variáveis contendo informações sobre o status da redacao, código da prova, gabaritos e respostas serão removidas porque não são relevantes para a análise, não imputarei valores nulos nelas.\n",
    "- Entre os principais objetivos da Análise de Desempenho, estão, entender o perfil dos candidatos que tiram determinadas notas, quais variáveis se relacionam com as notas e como estas podem ser utilizadas para predição. Portanto, candidatos faltantes que possuem notas nulas serão separados futuramente. Por enquanto, imputarei essas observações com nota zero. Isso será útil para analisar distribuições e fatores que influenciam o comparecimento (ou não) em cada dia de prova, compondo a Análise de Abstenção.\n",
    "- Para candidatos que foram eliminados, os quais também possuem nota nula nas provas, irei dropá-los de antemão, pois representam uma parcela muito pequena da população e não trazem informação relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_microdados = microdados.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping eliminated records.\n",
    "eliminated_records = clean_microdados.loc[(clean_microdados['TP_PRESENCA_CN'] == 2) \n",
    "                                          | (clean_microdados['TP_PRESENCA_CH'] == 2) \n",
    "                                          | (microdados['TP_PRESENCA_LC'] == 2) \n",
    "                                          | (microdados['TP_PRESENCA_MT'] == 2)].index\n",
    "clean_microdados = clean_microdados.drop(eliminated_records)\n",
    "\n",
    "# Imputing the grade as zero for those who didn't do the respective exam day.\n",
    "to_impute_grade = ['NU_NOTA_CN', 'NU_NOTA_MT', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_REDACAO', 'NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5']\n",
    "clean_microdados[to_impute_grade] = clean_microdados[to_impute_grade].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Removendo colunas desnecessárias\n",
    "- É possível perceber que temos um grande número de colunas. Observando o dicionário de variáveis, muitas dessas não nos interessam, e portanto, serão removidas. Entram nesse grupo:\n",
    "    - Variáveis informando códigos (códigos de prova para cada área do conhecimento, de cidades, unidades federativas, entre outros).\n",
    "    - Variáveis que podem introduzir viés na análise, como as que identificam cor e raça.\n",
    "    - Variáveis com gabaritos e respostas de questoes.\n",
    "    - Variáveis altamente desbalanceadas e que podem ser refletidas em valores de outras colunas, como o status da redação.\n",
    "    - Variáveis informando nacionalidade, número de inscrição e ano de conclusão do ensino médio. \n",
    "    - Variáveis com um identificador único para cada estudante.\n",
    "    - Variáveis contendo respostas para perguntas do questionário que não trazem valor para a análise ou que contêm múltiplas possibilidades para uma categoria.\n",
    "    - Variáveis com alto percentual de valores nulos, como visto acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns to drop.\n",
    "to_drop = ['NU_INSCRICAO', 'NU_ANO', 'TP_NACIONALIDADE', 'TP_COR_RACA', 'TP_ANO_CONCLUIU', \n",
    "           'CO_MUNICIPIO_ESC', 'NO_MUNICIPIO_ESC', 'CO_UF_ESC', 'SG_UF_ESC', 'TP_DEPENDENCIA_ADM_ESC', \n",
    "           'TP_LOCALIZACAO_ESC', 'TP_SIT_FUNC_ESC', 'TP_ENSINO', 'TP_STATUS_REDACAO', 'CO_MUNICIPIO_PROVA', 'CO_UF_PROVA', \n",
    "           'CO_PROVA_CN', 'CO_PROVA_CH', 'CO_PROVA_LC', 'CO_PROVA_MT', 'TX_RESPOSTAS_CN', 'TX_RESPOSTAS_CH', \n",
    "           'TX_RESPOSTAS_LC', 'TX_RESPOSTAS_MT', 'TX_GABARITO_CN', 'TX_GABARITO_CH', 'TX_GABARITO_LC', \n",
    "           'TX_GABARITO_MT', 'Q003', 'Q004', 'Q005', 'Q007', 'Q008', 'Q009', 'Q010', 'Q011', 'Q012', 'Q013', 'Q014', \n",
    "           'Q015', 'Q016', 'Q017', 'Q018', 'Q019', 'Q020', 'Q021', 'Q023']\n",
    "clean_microdados = clean_microdados.drop(columns=to_drop)\n",
    "clean_microdados.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ótimo! Já foi possível reduzir o número de variáveis de 76 para 29!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Alterando variáveis\n",
    "- De forma geral, variáveis de natureza categórica estão com tipos numéricos. Irei convertê-las com os respectivos valores categóricos a fim de tornar a análise interpretável e de fácil entendimento. Ao mesmo tempo em que converto, irei unir/dropar categorias, de acordo com o melhor critério.\n",
    "- Irei também padronizar os valores e renomear colunas para melhor entendimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardazing features names, formatting and renaming them.\n",
    "clean_microdados.columns = [x.lower() for x in clean_microdados.columns]\n",
    "\n",
    "prefixes_to_remove = ['tp_', 'in_', 'sg_', 'nu_', 'no_']\n",
    "clean_microdados.columns = clean_microdados.columns.to_series().replace(to_replace='^(' + '|'.join(prefixes_to_remove) + ')', value='', regex=True)\n",
    "\n",
    "to_rename = {'q001': 'escolaridade_pai', \n",
    "             'q002': 'escolaridade_mae',\n",
    "             'q003': 'ocupacao_pai',\n",
    "             'q004': 'ocupacao_mae',\n",
    "             'q006': 'renda_familiar_mensal',\n",
    "             'q022': 'possui_celular_em_casa',\n",
    "             'q024': 'possui_computador_em_casa',\n",
    "             'q025': 'acesso_internet_em_casa',\n",
    "             'st_conclusao': 'status_conclusao_ensino_medio'}\n",
    "\n",
    "clean_microdados = clean_microdados.rename(columns=to_rename)\n",
    "\n",
    "# Replacing numeric values by categorical values in categorical features in numeric data type. \n",
    "# Merging similar age categories into one, and assigning them a categorical value for better analysis interpretation.\n",
    "faixa_etaria_mapping = {\n",
    "    'Adolescente (< 18)': [1, 2],                   # < 18\n",
    "    'Jovem adulto (18-24)': [3, 4, 5, 6, 7, 8, 9],  # 18-24\n",
    "    'Adulto jovem (25-35)': [10, 11, 12],           # 25-35\n",
    "    'Adulto de meia idade (36-45)': [13, 14],       # 36-45\n",
    "    'Meia idade (46-55)': [15, 16],                 # 46-55\n",
    "    'Pré aposentadoria (56-65)': [17, 18],          # 56-65\n",
    "    'Idoso (> 66)': [19, 20]                        # > 66\n",
    "}\n",
    "\n",
    "replaced_faixa_etaria = dict()\n",
    "\n",
    "for group, keys in faixa_etaria_mapping.items():\n",
    "    for key in keys:\n",
    "        replaced_faixa_etaria[key] = group\n",
    "\n",
    "clean_microdados['faixa_etaria'] = clean_microdados['faixa_etaria'].replace(to_replace=replaced_faixa_etaria)\n",
    "\n",
    "# Replacing estado civil\n",
    "estado_civil_mapping = {\n",
    "    0: 'Não informado',\n",
    "    1: 'Solteiro(a)',\n",
    "    2: 'Casado(a)/União Estável',\n",
    "    3: 'Divorciado(a)/Separado(a)',\n",
    "    4: 'Viúvo(a)'\n",
    "}\n",
    "clean_microdados['estado_civil'] = clean_microdados['estado_civil'].replace(estado_civil_mapping)\n",
    "\n",
    "# Replacing high school conclusion situation.\n",
    "st_conclusao_mapping = {\n",
    "    1: 'Concluído',\n",
    "    2: 'Último ano',\n",
    "    3: 'Cursando',\n",
    "    4: 'Não concluído'\n",
    "}\n",
    "clean_microdados['status_conclusao_ensino_medio'] = clean_microdados['status_conclusao_ensino_medio'].replace(st_conclusao_mapping)\n",
    "\n",
    "# Replacing school type.\n",
    "escola_mapping = {\n",
    "    1: 'Não respondeu',\n",
    "    2: 'Pública',\n",
    "    3: 'Privada'\n",
    "}\n",
    "clean_microdados['escola'] = clean_microdados['escola'].replace(escola_mapping)\n",
    "\n",
    "# Replacing presenca and lingua.\n",
    "clean_microdados[['presenca_cn', 'presenca_ch', 'presenca_lc', 'presenca_mt']] = clean_microdados[['presenca_cn', 'presenca_ch', 'presenca_lc', 'presenca_mt']].replace({1: 'Presente', 0: 'Ausente'})\n",
    "clean_microdados['lingua'] = clean_microdados['lingua'].replace({0: 'Inglês', 1: 'Espanhol'})\n",
    "\n",
    "# Replacing questions answers.\n",
    "escolaridade_mapping = {\n",
    "    'A': 'Nunca estudou',\n",
    "    'B': 'Ensino fundamental incompleto',\n",
    "    'C': 'Ensino fundamental incompleto',\n",
    "    'D': 'Ensino fundamental completo',\n",
    "    'E': 'Ensino médio completo',\n",
    "    'F': 'Ensino superior completo',\n",
    "    'G': 'Pós-graduação',\n",
    "    'H': 'Não sei'\n",
    "}\n",
    "clean_microdados['escolaridade_pai'] = clean_microdados['escolaridade_pai'].replace(escolaridade_mapping)\n",
    "clean_microdados['escolaridade_mae'] = clean_microdados['escolaridade_mae'].replace(escolaridade_mapping)\n",
    "\n",
    "renda_mapping = {\n",
    "    'A': 'Nenhuma Renda',\n",
    "    'B': 'Até R$ 1.212,00',\n",
    "    'C': 'R$ 1.212,01 - R$ 1.818,00',\n",
    "    'D': 'R$ 1.818,01 - R$ 3.030,00',\n",
    "    'E': 'R$ 1.818,01 - R$ 3.030,00',\n",
    "    'F': 'R$ 3.030,01 - R$ 4.848,00',\n",
    "    'G': 'R$ 3.030,01 - R$ 4.848,00',\n",
    "    'H': 'R$ 4.848,01 - R$ 7.272,00',\n",
    "    'I': 'R$ 4.848,01 - R$ 7.272,00',\n",
    "    'J': 'R$ 7.272,01 - R$ 10.908,00',\n",
    "    'K': 'R$ 7.272,01 - R$ 10.908,00',\n",
    "    'L': 'R$ 7.272,01 - R$ 10.908,00',\n",
    "    'M': 'R$ 10.908,01 - R$ 18.180,00',\n",
    "    'N': 'R$ 10.908,01 - R$ 18.180,00',\n",
    "    'O': 'R$ 10.908,01 - R$ 18.180,00',\n",
    "    'P': 'R$ 18.180,01 - R$ 24.240,00',\n",
    "    'Q': 'Acima de R$ 24.240,00'\n",
    "}\n",
    "clean_microdados['renda_familiar_mensal'] = clean_microdados['renda_familiar_mensal'].replace(renda_mapping)\n",
    "\n",
    "computador_celular_mapping = {\n",
    "    'A': 'Não',\n",
    "    'B': 'Um',\n",
    "    'C': 'Dois ou mais',\n",
    "    'D': 'Dois ou mais',\n",
    "    'E': 'Dois ou mais'\n",
    "}\n",
    "clean_microdados['possui_celular_em_casa'] = clean_microdados['possui_celular_em_casa'].replace(computador_celular_mapping)\n",
    "clean_microdados['possui_computador_em_casa'] = clean_microdados['possui_computador_em_casa'].replace(computador_celular_mapping)\n",
    "\n",
    "internet_mapping = {\n",
    "    'A': 'Não', \n",
    "    'B': 'Sim'\n",
    "}\n",
    "clean_microdados['acesso_internet_em_casa'] = clean_microdados['acesso_internet_em_casa'].replace(internet_mapping)\n",
    "\n",
    "treineiro_mapping = {\n",
    "    0: 'Não',\n",
    "    1: 'Sim'\n",
    "}\n",
    "clean_microdados['treineiro'] = clean_microdados['treineiro'].replace(treineiro_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Otimização de memória\n",
    "- Irei converter os tipos das variáveis para tipos que consomem menos memória sem perder informação, a fim de otimizar a análise e manipulação dos dados. \n",
    "- Após isso irei salvar o dataset resultante em um formato parquet, uma vez que o csv não mantém os tipos convertidos. \n",
    "- Uma vez que as notas vão de 0 a 1000 e têm precisão de 1 casa decimal, converterei-as de float64 para float32. Não será utilizado float16 pois o pyarrow não suporta esse tipo e não será possível salvar em parquet.\n",
    "- Variáveis com um número limitado de categorias exclusivas serão convertidas para category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining and converting columns to lower memory consumption data types.\n",
    "to_float32 = clean_microdados.select_dtypes('float64').columns.tolist()\n",
    "to_category = clean_microdados.select_dtypes('object').columns.tolist()\n",
    "to_category.remove('municipio_prova')\n",
    "\n",
    "clean_df = clean_microdados.copy()\n",
    "clean_df[to_float32] = clean_df[to_float32].astype('float32')\n",
    "clean_df[to_category] = clean_df[to_category].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the memory optimized data to a parquet file in order to maintain the converted data types.\n",
    "path = 'D:\\\\MLProjects\\\\DadosEnem\\\\clean_df.parquet'\n",
    "clean_df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the memory optimized data.\n",
    "path = 'D:\\\\MLProjects\\\\DadosEnem\\\\clean_df.parquet'\n",
    "df = pd.read_parquet(path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Conclusão\n",
    "- Excelente! Através da limpeza dos dados foi possível reduzir o tamanho do dataset de +2 GB para +218.4 MB, quase 10%! Agora poderemos realizar a análise e manipulação dos dados de forma eficiente.\n",
    "- Tarefas realizadas:\n",
    "    -  Identificação e tratamento de valores nulos e duplicados, de acordo com os objetivos da análise.\n",
    "    -  Remoção de variáveis irrelevantes para a análise.\n",
    "    -  Feature engineering: Criação e alteração de variáveis existentes. Aqui, irei fundir, remover e renomear categorias com base na melhor formatação para o meu objetivo. Além disso, converter colunas para o tipo de dado correto também será importante.\n",
    "    -  Otimização de memória: Conversão de variáveis a tipos de dados menores, a fim de melhorar a performance, possibilitando a leitura e manipulação dos dados em menor tempo, sem que haja a perda de informação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
